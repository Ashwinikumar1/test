#!/usr/bin/env python
# coding: utf-8

import sys
import os
import shutil
import pandas as pd
import sys
import glob
import tqdm
import yaml
import argparse
import torch
import IProgress
import ipywidgets
sys.path.append('../')

from data_loader.data_preprocessing import *
from configs.config import config
from models.doduo.train_multi import *
from models.doduo.predict_multi import *

# Define the command-line arguments
parser = argparse.ArgumentParser(description='Optimus Training Pipeline')
parser.add_argument('--input_dir', type=str, help='Path to the input data directory')
parser.add_argument('--mapping_file', type=str, help='Path to the mapping file')
args = parser.parse_args()

# Load the configuration
config_path = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/optimus-aml-gpu/code/Users/ashwini_kumar/optimus_training_pipeline/data_transformer_training_pipeline/configs/config.yaml'
with open(config_path, 'r') as f:
    config_dict = yaml.safe_load(f)

# Update the input_dir and mapping_file_variable if provided as arguments
if args.input_dir is not None:
    config_dict['input_dir'] = args.input_dir

if args.mapping_file is not None:
    config_dict['mapping_file_variable'] = args.mapping_file

# Save the updated config dictionary to the file
with open(config_path, 'w') as f:
    yaml.dump(config_dict, f)

# Check if the output directory already exists
if os.path.exists(os.path.join(config_dict['output_dir'], config_dict['task_id'])):
    print("Data directory exists.")
else:
    # Create the output directory
    os.makedirs(os.path.join(config_dict['output_dir'], config_dict['task_id']))
    print("Data directory created.")

# Preprocess the data
out = create_new_cv_data(config_dict['input_dir'], config_dict['cv_list'], nsample=config_dict['nsample'])

# Add the coltypes and num_classes on the go in config class so it can be used anywhere
coltypes, num_classes = extract_class(out[0])
config_dict['coltypes'] = coltypes
config_dict['num_classes'] = num_classes

print(f"Number of classes for training: {num_classes}")
print(f"The classes are as follows: {coltypes}")

# Load the model inputs file
model_inputs_path = './models/doduo/model_inputs.txt'
with open(model_inputs_path, 'r') as f:
    lines = f.readlines()

# Loop over the lines in the file and update the variables as needed
for i, line in enumerate(lines):
    if line.startswith('sato_coltypes'):
        lines[i] = f'sato_coltypes: {coltypes}\n'
    elif line.startswith('num_classes'):
        lines[i] = f'num_classes: {num_classes}\n'

# Save the updated model inputs file
with open(model_inputs_path, 'w') as f:
    f.writelines(lines)

# Check if CUDA is available
if torch.cuda.is_available():
    print("CUDA is available.")
else:
    print("CUDA is not available.")

# Train the model
train_multi(config_dict)

# Predict using the trained model
predict_multi(config_dict)


#!/usr/bin/env python
# coding: utf-8

import sys
import os
import shutil
import pandas as pd
import sys
import glob
import tqdm
import argparse
sys.path.append('../')

from data_loader.data_preprocessing import *
from configs.config import config

# Parse command-line arguments
parser = argparse.ArgumentParser(description='Training pipeline')
parser.add_argument('--input_dir', required=True, help='Input directory')
parser.add_argument('--mapping_file', required=True, help='Mapping file')
args = parser.parse_args()

# Load the configuration file
config = config('/mnt/batch/tasks/shared/LS_root/mounts/clusters/optimus-aml-gpu/code/Users/ashwini_kumar/optimus_training_pipeline/data_transformer_training_pipeline/configs/config.yaml')

# Update the configuration file with command-line arguments
config.INPUT_DIR = args.input_dir
config.Mapping_file_variable = args.mapping_file

# Check if the repository already exists
if os.path.exists(os.path.join(config.OUTPUT_DIR, config.TASK_ID)):
    print("Data Directory exist")
else:
    # Create the repository
    os.makedirs(os.path.join(config.OUTPUT_DIR, config.TASK_ID))
    print("Data Directory created..")

# Create a new cross-validation dataset
out = create_new_cv_data(config.INPUT_DIR, config.CV_LIST, nsample=config.NSAMPLE)

# Extract class labels and column types
coltypes,num_classes = extract_class(out[0])

# Update the config with column types and number of classes
config.COLTYPES = coltypes
config.NUM_CLASSES = num_classes

# Print number of classes and column types
print("Number of classes for training are :", config.NUM_CLASSES)
print("The classes are as follows :", config.COLTYPES)

# Load the text file containing the model inputs
with open('./models/doduo/model_inputs.txt', 'r') as f:
    lines = f.readlines()

# Update the variables in the text file
for i, line in enumerate(lines):
    if line.startswith('sato_coltypes'):
        lines[i] = f'sato_coltypes: {coltypes}\n'
    elif line.startswith('num_classes'):
        lines[i] = f'num_classes: {num_classes}\n'

# Save the updated text file
with open('./models/doduo/model_inputs.txt', 'w') as f:
    f.writelines(lines)

# Check if a GPU is available
import torch
if torch.cuda.is_available():
    print("GPU available")
else:
    print("GPU not available")

# Run the training script
import IProgress
import ipywidgets
get_ipython().run_line_magic('run', 'models/doduo/train_multi.py')

# Run the prediction script
get_ipython().run_line_magic('run', 'models/doduo/predict_multi.py')


#!/usr/bin/env python
# coding: utf-8

# In[1]:


#!/usr/bin/env python
# coding: utf-8

import sys
import os
import shutil
import pandas as pd
import sys
import glob
import tqdm
sys.path.append('../')

from data_loader.data_preprocessing import *
from configs.config import config
import argparse

# load the configuration
config = config('/mnt/batch/tasks/shared/LS_root/mounts/clusters/optimus-aml-gpu/code/Users/ashwini_kumar/optimus_training_pipeline/data_transformer_training_pipeline/configs/config.yaml')



# Check if the repository already exists
if os.path.exists(os.path.join(config.OUTPUT_DIR, config.TASK_ID)):
    print("Data Directory exist")
else:
    # Create the repository
    os.makedirs(os.path.join(config.OUTPUT_DIR, config.TASK_ID))
    print("Data Directory created..")


# In[2]:


out = create_new_cv_data(config.INPUT_DIR, config.CV_LIST, nsample=config.NSAMPLE)


# In[3]:


# Add the coltypes and num classes on the go in config class so it can be used anywhere
coltypes,num_classes = extract_class(out[0])

config.COLTYPES = coltypes
config.NUM_CLASSES = num_classes

print (" Number of classes for training are :", config.NUM_CLASSES)
print (" The classes are as follows :",config.COLTYPES)

# Load the text file
with open('./models/doduo/model_inputs.txt', 'r') as f:
     lines = f.readlines()

# # Loop over the lines in the file and update the variables as needed
for i, line in enumerate(lines):
     if line.startswith('sato_coltypes'):
          lines[i] = f'sato_coltypes: {coltypes}\n'
     elif line.startswith('num_classes'):
          lines[i] = f'num_classes: {num_classes}\n'


# # Save the updated text file
with open('./models/doduo/model_inputs.txt', 'w') as f:
     f.writelines(lines)


# In[4]:


import torch
torch.cuda.is_available()


# In[5]:


import IProgress
import ipywidgets
get_ipython().run_line_magic('run', 'models/doduo/train_multi.py')


# In[7]:


get_ipython().run_line_magic('run', 'models/doduo/predict_multi.py')

import subprocess

# Define the command to run the script
command = "python models/doduo/predict_multi.py"

# Run the command
subprocess.run(command, shell=True)
